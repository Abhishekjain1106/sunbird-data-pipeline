kafka {
  input.topic = "k8s.telemetry.ingest"
  output.success.topic = "k8s.telemetry.raw.flink"
  output.duplicate.topic = "k8s.telemetry.duplicate.flink"
  output.malformed.topic = "k8s.telemetry.malformed.flink"
  output.failed.topic = "k8s.telemetry.failed.flink"
  broker-servers = "1192.168.43.149:9092"
  zookeeper = "192.168.43.149:2181"
  event.max.size = "2000"
  # dev-environment
  # broker-servers = "11.2.1.15:9092"
  # zookeeper = "11.2.1.15:2181"
  groupId = "telemetry-extractor-group"
}

task {
  parallelism = 3
  checkpointing.interval = 60000
}

dedup.validation.required = "true"

redis {
  host = 127.0.0.1
  # dev-environment
  # host = 11.2.4.22
  port = 6379
  connection {
    max = 2
    idle.min = 1
    idle.max = 2
    minEvictableIdleTimeSeconds = 120
    timeBetweenEvictionRunsSeconds = 300
  }
  database {
    duplicationstore.id = 12
    key.expiry.seconds = 3600
  }
}