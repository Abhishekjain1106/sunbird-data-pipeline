kafka {
  input.topic = "k8s.telemetry.denorm"
  output.telemetry.route.topic = "k8s.events.telemetry.flink"
  output.summary.route.topic = "k8s.events.summary.flink"
  output.log.route.topic = "k8s.events.log.flink"
  output.error.route.topic = "k8s.events.error.flink"
  output.failed.topic = "k8s.telemetry.failed.flink"
  output.duplicate.topic = "k8s.telemetry.duplicate.flink"
  broker-servers = "127.0.0.1:9092"
  zookeeper = "127.0.0.1:2181"
  groupId = "druid-validator-group"
  # dev-environment
  # broker-servers = "11.2.1.15:9092"
  # zookeeper = "11.2.1.15:2181"
}

task {
  parallelism = 1
  checkpointing.interval = 60000
  metrics.window.size = 5000 # 5 seconds for test cases
  validator {
    parallelism = 1
  }
  router {
    parallelism = 1
  }
}

schema {
  path {
    telemetry = "schemas/telemetry"
    summary = "schemas/summary"
  }
  file {
    default = envelope.json
    summary = me_workflow_summary.json
    search = search.json
  }
}

redis {
  host = 127.0.0.1
  port = 6379
  connection {
    max = 2
    idle.min = 1
    idle.max = 2
    minEvictableIdleTimeSeconds = 120
    timeBetweenEvictionRunsSeconds = 300
  }
  database {
    duplicationstore.id = 10
    key.expiry.seconds = 3600
  }
}