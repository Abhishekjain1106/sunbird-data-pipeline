#!/usr/bin/env bash

export SPARK_HOME={{ analytics.home }}/spark-{{ spark_version }}-bin-hadoop2.7
export MODELS_HOME={{ analytics.home }}/models-{{ model_version }}
export DP_LOGS={{ analytics.home }}/logs/data-products
export KAFKA_HOME={{ analytics.home }}/kafka_2.11-0.10.1.0

## Job to run daily
cd /mount/data/analytics/scripts
source model-config.sh
today=$(date "+%Y-%m-%d")

## job broker-list and kafka-topic
zookeeper={{ zookeeper }}
brokerList={{ brokerlist }}
job_topic={{ analytics_job_queue_topic }}
unique_topic={{ env }}.telemetry.unique

type=$2
start_date=$5
end_date=$6 
if [ -z "$7" ]; then sparkMaster="local[*]"; else sparkMaster="$7"; fi

if [ "$type" = "via-partition" ]; then
    # get partitions from kafka metadata
    if [ -z "$3" ]; then partitions=`$KAFKA_HOME/bin/kafka-topics.sh --zookeeper $zookeeper --describe --topic $unique_topic   | awk 'NR==1{print $2}' |  awk -F":" '{print $2}'`; else partitions=$3; fi
    endPartitions=`expr $partitions - 1`
    if [ -z "$4" ]; then parallelisation=1; else parallelisation=$4; fi
    # add partitions to config and start jobs
    for i in $(seq 0 $parallelisation $endPartitions)
        do 
            # add partitions to config
            partitionString="\"delta\":0,\"partitions\":[$(seq -s , $i `expr $i + $parallelisation - 1`)]"
            if [ -z "$start_date" ]; then
                job_config=$(config $1)
                finalConfig=${job_config/'"delta":0'/$partitionString}
                echo $finalConfig
                echo "Running $1 by partitions."
                nohup $SPARK_HOME/bin/spark-submit --master $sparkMaster --jars $(echo ${libs_path}/lib/*.jar | tr ' ' ','),$MODELS_HOME/analytics-framework-2.0.jar,$MODELS_HOME/scruid_2.11-2.3.2.jar,$MODELS_HOME/batch-models-2.0.jar --class org.ekstep.analytics.job.JobExecutor $MODELS_HOME/batch-models-2.0.jar --model "$1" --config "$finalConfig" >> "$DP_LOGS/$today-job-execution.log" 2>&1
            else 
                job_config=$(config $1 '__endDate__')
                finalConfig=${job_config/'"delta":0'/$partitionString}
                echo $finalConfig
                echo "Running $1 by partitions via Replay-Supervisor."
                nohup $SPARK_HOME/bin/spark-submit --master $sparkMaster --jars $MODELS_HOME/analytics-framework-2.0.jar,$MODELS_HOME/scruid_2.11-2.3.2.jar --class org.ekstep.analytics.job.ReplaySupervisor $MODELS_HOME/batch-models-2.0.jar --model "$1" --fromDate "$start_date" --toDate "$end_date" --config "$finalConfig" >> "$DP_LOGS/$end_date-$1-replay.log" 2>&1
            fi
        done
elif [ "$type" = "job-manager" ]; then
    # push job-config to kafka      
    echo "Submitting $1 to job-manager."
    echo '{ "model" :' \"$1\" ',' ' "config": ' "$job_config" '}' > /tmp/job-request.json
    cat /tmp/job-request.json | $KAFKA_HOME/bin/kafka-console-producer.sh --broker-list $brokerList --topic $job_topic >> "$DP_LOGS/$today-job-execution.log" 2>&1
else
    if [ -z "$start_date" ]; then
        echo "Running $1 without partition via run-job."
        job_config=$(config $1)
        nohup $SPARK_HOME/bin/spark-submit --master $sparkMaster --jars $(echo ${libs_path}/lib/*.jar | tr ' ' ','),$MODELS_HOME/analytics-framework-2.0.jar,$MODELS_HOME/scruid_2.11-2.3.2.jar,$MODELS_HOME/batch-models-2.0.jar --class org.ekstep.analytics.job.JobExecutor $MODELS_HOME/batch-models-2.0.jar --model "$1" --config "$job_config" >> "$DP_LOGS/$today-job-execution.log" 2>&1
    else   
        job_config=$(config $1 '__endDate__')
        echo "Running $1 without partition via Replay-Supervisor." 
        nohup $SPARK_HOME/bin/spark-submit --master $sparkMaster --jars $MODELS_HOME/analytics-framework-2.0.jar,$MODELS_HOME/scruid_2.11-2.3.2.jar --class org.ekstep.analytics.job.ReplaySupervisor $MODELS_HOME/batch-models-2.0.jar --model "$1" --fromDate "$start_date" --toDate "$end_date" --config "$job_config" >> "$DP_LOGS/$end_date-$1-replay.log" 2>&1
    fi    
fi